20200608

在网上看到一道题，感觉还挺好的，比较贴近业务。原文是用spark写的，我打算用sql和spark都试一下，整理一下看能不能写出来：



### 题目

给定一张表，两个字段，用户id和用户登录的日期。假设业务开始时间从2020年1月1日开始，即1月1日登录的都是新用户。要求输出每天的新增用户。示例数据如下：

```
2020-01-01	a
2020-01-01	b
2020-01-01	c
-----------------
2020-01-02	a(不是新用户)
2020-01-02	b(不是新用户)
2020-01-02	d
----------------
2020-01-03	b(不是新用户)
2020-01-03	e
2020-01-03	f
```

以上数据要求的输出结果为：
2020-01-01 新增三个用户（a,b,c）
2020-01-02 新增一个用户（d）
2020-01-03 新增两个用户（e，f）



### sql解题思路

额，又没想出来。。。

[a,b,c]

[a,b,d]

感觉应该有个累加变量存储截止到某一天为止已经出现过的全部用户。

-------------------------------------------

又问了sql大神同事，果然大神就是大神，为什么我就没想出来呢？感觉我已经被循环、遍历等python操作禁锢住了。

他的思路就是**每个用户只要取最小的登录时间**即可，相当于先去重，然后按照日期group by 就是每天第一次登陆的用户了。

我真的没有想到这个思路。。。没转过弯来。。。

突然发现这题和第一次访问的用户那题不是一样么？我脑子抽了这道题一下子没反应过来？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？



### python解题思路

```python
def new_users(user_log):
    """
    user_log like ['2020-01-01#a', '2020-01-01#b']
    """
    # 初始化
    prev_date = {}
    users = set()
    
    for log in user_log:
        date = log.split("#")[0]
        user_id = log.split("#")[1]
        
        if date not in prev_date.keys():
            prev_date[date] = []
        
        if user_id not in users:
            prev_date[date].append(user_id)
        
        # 把用户加入旧用户的set中
        users.add(user_id)
        
    return prev_date
```

测试： new_users(['2020-01-01#a', '2020-01-01#b','2020-01-02#b'])

输出：{'2020-01-01': ['a', 'b'], '2020-01-02': []}



### spark 操作

20200610

```python
import os
from pyspark.sql import SparkSession

# Python binary executable to use for PySpark in both driver and workers (default is python2.7 if available, otherwise python).
os.environ['PYSPARK_PYTHON']="/usr/local/anaconda3/bin/python3.6"

spark = SparkSession.builder.master('local').appName('test').getOrCreate()

# 创建一个测试rdd
rdd  = spark.sparkContext.parallelize([
    ("2017-01-01","a"),("2017-01-01","b"),
    ("2017-01-02","a"),("2017-01-02","c"),
    ("2017-01-03","c"),("2017-01-03","d")])


def new_users(rdd):
    """
    整体思路还是先求每个用户的最小登录日期
    然后根据登录日期作为key求用户id的聚合list作为该日期的新用户
    """
    print("original rdd is like")
    print(rdd.take(5))
    print("="*10)
    
    # key:value 交换顺序（倒排）变为 value:key，即 用户id:登录日期
    rdd2 = rdd.map(lambda kv: (kv[1], kv[0]))
    print("step1")
    print(rdd2.take(num = 5))
    print("="*10)

    # 按照key进行聚合，把value聚合成list，类似于hive中的collect_list()，即用户id:[登录日期1，登陆日期2，登陆日期3...]
    rdd3 = rdd2.groupByKey().mapValues(list)
    print("step2")
    print(rdd3.take(5))
    print("="*10)

    # 取每个用户id的最小登陆日期 
    # 1.使用map函数
    rdd4 = rdd3.map(lambda kv: (kv[0],min(kv[1])))
    print("step3")
    print(rdd4.take(5))
    # 2.或者是使用mapValues函数
    rdd4_hat = rdd3.mapValues(lambda value: min(value))
    print("step3")
    print(rdd4_hat.take(5))
    print("="*10)

    # 再一次倒排，把数据换成 登录日期:用户id这种形式
    rdd5 = rdd4.map(lambda kv:(kv[1],kv[0]))
    print("step4")
    print(rdd5.take(5))
    print("="*10)

    # 以登录日期为key对用户id求聚合
    rdd6 = rdd5.groupByKey( ).mapValues(list)
    print("step5")
    print(rdd6.take(5))
    print("="*10)
    
    return rdd6.collect()
    
    
    
# 如果对spark的各种行动转化操作更熟练一些，代码可以更加精炼，可以使用函数式编程的思维把一些步骤合并
def new_users_concise(rdd):
    """
    整体思路还是先求每个用户的最小登录日期
    然后根据登录日期作为key求用户id的聚合list作为该日期的新用户
    """
    print("original rdd is like")
    print(rdd.take(5))
    print("="*10)
    
    # 倒排 聚合
    rdd2 = rdd.map(lambda kv: (kv[1], kv[0])).groupByKey().mapValues(list)
    print(rdd2.take(5))
    print("="*10)
    
    # 取每个用户id的最小登陆日期 
    rdd3 = rdd2.mapValues(lambda value: min(value))
    print(rdd3.take(5))
    print("="*10)

    # 再一次倒排# 倒排 聚合
    rdd4 = rdd3.map(lambda kv:(kv[1],kv[0])).groupByKey( ).mapValues(list)
    print(rdd4.take(5))
    print("="*10)
    
    return rdd4.collect()
```

下面是测试结果：

```
new_users(rdd)

original rdd is like
[('2017-01-01', 'a'), ('2017-01-01', 'b'), ('2017-01-02', 'a'), ('2017-01-02', 'c'), ('2017-01-03', 'c')]
==========
step1
[('a', '2017-01-01'), ('b', '2017-01-01'), ('a', '2017-01-02'), ('c', '2017-01-02'), ('c', '2017-01-03')]
==========
step2
[('a', ['2017-01-01', '2017-01-02']), ('b', ['2017-01-01']), ('c', ['2017-01-02', '2017-01-03']), ('d', ['2017-01-03'])]
==========
step3
[('a', '2017-01-01'), ('b', '2017-01-01'), ('c', '2017-01-02'), ('d', '2017-01-03')]
step3
[('a', '2017-01-01'), ('b', '2017-01-01'), ('c', '2017-01-02'), ('d', '2017-01-03')]
==========
step4
[('2017-01-01', 'a'), ('2017-01-01', 'b'), ('2017-01-02', 'c'), ('2017-01-03', 'd')]
==========
step5
[('2017-01-01', ['a', 'b']), ('2017-01-02', ['c']), ('2017-01-03', ['d'])]
==========
[('2017-01-01', ['a', 'b']), ('2017-01-02', ['c']), ('2017-01-03', ['d'])]
```

```python
new_users_concise(rdd)

original rdd is like
[('2017-01-01', 'a'), ('2017-01-01', 'b'), ('2017-01-02', 'a'), ('2017-01-02', 'c'), ('2017-01-03', 'c')]
==========
[('a', ['2017-01-01', '2017-01-02']), ('b', ['2017-01-01']), ('c', ['2017-01-02', '2017-01-03']), ('d', ['2017-01-03'])]
==========
[('a', '2017-01-01'), ('b', '2017-01-01'), ('c', '2017-01-02'), ('d', '2017-01-03')]
==========
[('2017-01-01', ['a', 'b']), ('2017-01-02', ['c']), ('2017-01-03', ['d'])]
==========
[('2017-01-01', ['a', 'b']), ('2017-01-02', ['c']), ('2017-01-03', ['d'])]
```



原文链接

https://dongkelun.com/2018/04/11/sparkNewUV/