20200608

在网上看到一道题，感觉还挺好的，比较贴近业务。原文是用spark写的，我打算用sql和spark都试一下，整理一下看能不能写出来：



### 题目

给定一张表，两个字段，用户id和用户登录的日期。假设业务开始时间从2020年1月1日开始，即1月1日登录的都是新用户。要求输出每天的新增用户。示例数据如下：

```
2020-01-01	a
2020-01-01	b
2020-01-01	c
-----------------
2020-01-02	a(不是新用户)
2020-01-02	b(不是新用户)
2020-01-02	d
----------------
2020-01-03	b(不是新用户)
2020-01-03	e
2020-01-03	f
```

以上数据要求的输出结果为：
2020-01-01 新增三个用户（a,b,c）
2020-01-02 新增一个用户（d）
2020-01-03 新增两个用户（e，f）



### sql解题思路

额，又没想出来。。。

[a,b,c]

[a,b,d]

感觉应该有个累加变量存储截止到某一天为止已经出现过的全部用户。

-------------------------------------------

又问了sql大神同事，果然大神就是大神，为什么我就没想出来呢？感觉我已经被循环、遍历等python操作禁锢住了。



他的思路就是**每个用户只要取最小的登录时间**即可，相当于先去重，然后按照日期group by 就是每天第一次登陆的用户了。

我真的没有想到这个思路。。。没转过弯来。。。





### python解题思路

```
def new_users(user_log):
    """
    user_log like ['2020-01-01#a', '2020-01-01#b']
    """
    # 初始化
    prev_date = {}
    users = set()
    
    for log in user_log:
        date = log.split("#")[0]
        user_id = log.split("#")[1]
        
        if date not in prev_date.keys():
            prev_date[date] = []
        
        if user_id not in users:
            prev_date[date].append(user_id)
        
        # 把用户加入旧用户的set中
        users.add(user_id)
        
    return prev_date
```

测试： new_users(['2020-01-01#a', '2020-01-01#b','2020-01-02#b'])

输出：{'2020-01-01': ['a', 'b'], '2020-01-02': []}



### spark 操作

-- 待填坑---



原文链接

https://dongkelun.com/2018/04/11/sparkNewUV/